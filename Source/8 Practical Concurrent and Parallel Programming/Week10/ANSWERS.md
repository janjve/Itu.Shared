Week 10
-------------
Authors: Jan Vium Enghoff, SÃ¸ren Harisson

Exercise 10.1
=============

10.1.1
-------------

- `increment`: This is the default CAS implementation. We only increment the value if it was not changed by another thread during the operation.

- `getBins`: The documentation for AtomicInteger ensures us that we have visibility when using get(). We never update in this method.

- `getAndClear`: The reason this works is the same as for `increment`. When the old value is returned from getAndClear it is not given that the count is still 0, as it might have been updated again after the compareAndSet call.

- `transferBins`: This works because we don't care if we have the newest value from `hist` as long as it is in a consistent state. We use `getAndClear` we wrote earlier for this. We do however have to be sure that there were no changes to the bin in `this`. By using compareAndSet we achieve this.

10.1.2
-------------

We run the test from last week with transfering bins. This produce the following output:

```
   0:         0
   1:         0
   2:         0
   3:         0
   4:         0
   5:         0
   6:         0
   7:         0
   8:         0
   9:         0
  10:         0
  11:         0
  12:         0
  13:         0
  14:         0
  15:         0
  16:         0
  17:         0
  18:         0
  19:         0
  20:         0
  21:         0
  22:         0
  23:         0
  24:         0
  25:         0
  26:         0
  27:         0
  28:         0
  29:         0
              0
   0:         2
   1:    283146
   2:    790986
   3:    988651
   4:    810386
   5:    524171
   6:    296702
   7:    155475
   8:     78002
   9:     38069
  10:     18232
  11:      8656
  12:      4055
  13:      1886
  14:       865
  15:       400
  16:       179
  17:        79
  18:        35
  19:        14
  20:         7
  21:         2
  22:         0
  23:         0
  24:         0
  25:         0
  26:         0
  27:         0
  28:         0
  29:         0
        4000000
```

This is as expected. All the values has been transfered and it gives the correct result.

10.1.3
-------------

To do this measurement we will remove the transfer part of the test.

The implementation for STM is included in `.\src\old(10.1.3,10.1.4)\TestStmHistogram.java`.

STM:
```
Runningtime:      1.21116177 s
```

CAS:
```
Runningtime:      1.20353944 s
```

We ran this multiple times and they seem to be similar in runtime.


10.1.4
------------

The implementation for lock based is included in `.\src\old(10.1.3,10.1.4)\SimpleHistogram.java`.

Lock based:
```
Runningtime:      1.97263442 s
```

There is a signicant performance gain by using the CAS or STM implementations.

Exercise 10.2
=============

The implementation can be found in `.\src\TestSimpleRWTryLock.java`.

Exercise 10.3
=============

The measurements are obtained by running the file `.\src\TestPseudoRandom.java`.

See `.\result\10.3.1-jve-plot.png` for visualization of the results. 
`.\result\10.3.1-jve-plot-TL.png` is from the same testrun, but only showing the threadlocal times.

The test identified the following system information.

```
# OS:   Windows 10; 10.0; amd64
# JVM:  Oracle Corporation; 1.8.0_111
# CPU:  Intel64 Family 6 Model 60 Stepping 3, GenuineIntel; 8 "cores"
# Date: 2016-11-12T13:16:27+0100
```

Additional information about the system:

```
# Processor: Intel(R) Core(TM) i7-4712HQ CPU @ 2.30GHz
# Installed memory (RAM): 16.0 GB
# System type: 64-bit Operating System, x64 based Processor
```

As extected there is a huge performance gain when using multiple threads until the number of physical cores.

The fastest of the five is java's own implementation of `ThreadLocalRandom`. This is the case for all number of threads.
Following that we have the two thread local versions. The reason for this is that there is no shared fields between the threads and they can therefor run in parallel without having to synchronize.
The worst performing RNGs are not surprisingly the two implementation using a shared seed. Synchronization across threads are needed to make sure the same seed isn't passed multiple times in a row. The best performing of the two is the optimistic approach. This is true for every number of threads for this paticular test. The time saved from not running unnecessary operations using a pessimistic lock simply does not outweigh the overhead generated by having to synchronize even two threads. 

What should also be noted is that as soon as we hit the number of physical cores in the system, the pessimistic implementation doesn't get worse running more threads. This is not the case with the pessimistic lock where starting more threads still generates some additional overhead.